[
  {
    "objectID": "biography.html",
    "href": "biography.html",
    "title": "Biography",
    "section": "",
    "text": "Daniel is Associate Professor of Statistics at the University of British Columbia in Vancouver. Before moving North, Daniel spent 8 years on the faculty at Indiana University, Bloomington. His research interests involve the estimation and quantification of prediction risk, especially developing methods for evaluating the predictive abilities of complex dependent data. This includes the application of statistical learning techniques to time series prediction problems, as well as investigations of cross-validation and the bootstrap for risk estimation.\nDaniel did his undergraduate studies at Indiana University where he received a Bachelor of Science in Music with a concentration in cello performance from the Jacobs School of Music and a Bachelor of Arts in economics and mathematics. Prior to graduate school, he worked as a Research Associate at the Federal Reserve Bank of St. Louis. He received his Ph.D. from Carnegie Mellon University in Statistics where he was named graduate student of the year for 2012 and received the Umesh Gavasakar Memorial Thesis Award for his dissertation “Generalization Error Bounds for State Space Models.” In 2017, he was a recipient of the Indiana University Trustees Teaching Award. In 2018, he received an NSF CAREER award. His work has also been supported by grants from the Institute for New Economic Thinking, the Canadian Statistical Sciences Institute, and the National Sciences and Engineering Research Council of Canada.\nSince the beginning of the COVID-19 pandemic, much of Daniel’s applied work has focused on methods for understanding and modelling epidemiological data. He is a core member of the BC COVID-19 Modelling Group and works on research, forecasting, nowcasting, and software development with Carnegie Mellon University’s Delphi Research Group."
  },
  {
    "objectID": "teaching/inet2015.html#instructor",
    "href": "teaching/inet2015.html#instructor",
    "title": "INET 2015",
    "section": "Instructor",
    "text": "Instructor\nDaniel McDonald\nDepartment of Statistics\nIndiana University, Bloomington"
  },
  {
    "objectID": "teaching/inet2015.html#course-description",
    "href": "teaching/inet2015.html#course-description",
    "title": "INET 2015",
    "section": "Course description",
    "text": "Course description\nLargely unnoticed by economists, over the last three decades statisticians and computer scientists have developed sophisticated prediction procedures and methods of model selection and forecast evaluation under the rubric of statistical learning theory. These methods have revolutionized pattern recognition and artificial intelligence, and the modern industry of data mining (without the pejorative connotation) would not exist without it.\nIn this short course, we will investigate the connections between modern statistical methodology and standard techniques which are more common in economics and social science. The goal is to develop an understanding for the circumstances under which different statistical methods are appropriate, how methods behave when the model for the data generating process is incorrect, and why some methods can adapt while others cannot. We will focus especially on selecting models with good predictive performance and on the relationships between model complexity, statistical estimation, and the amount of data used to estimate the model.\nThe starting point for statistical learning is the notion of predictive risk and the tradeoff between bias and variance. With this tradeoff in mind, we will investigate the importance of assessing models using training and test data, the benefits of regularization, and the necessity of selecting tuning parameters carefully. We will illustrate each of these issues with some standard econometric procedures applied to financial and economic datasets while at the same time, introducing some potentially new procedures which may be useful in your own research."
  },
  {
    "objectID": "teaching/inet2015.html#schedule-of-topics",
    "href": "teaching/inet2015.html#schedule-of-topics",
    "title": "INET 2015",
    "section": "Schedule of topics",
    "text": "Schedule of topics\n\nThe predictive viewpoint\nThe bias-variance tradeoff\nEvaluating predictions and estimators\nThe benefits of regularization\nModel selection\nChoosing tuning parameters\nApplication: BVARs and DSGEs\nTools for classification\nApplication: Predicting recessions\nCollaborative filtering and the Netflix prize\nDimension reduction"
  },
  {
    "objectID": "teaching/inet2015.html#suggested-preparation",
    "href": "teaching/inet2015.html#suggested-preparation",
    "title": "INET 2015",
    "section": "Suggested preparation",
    "text": "Suggested preparation\nIn order to be best prepared for this course, we suggest that you brush up on your econometrics, especially the content about probability. Most useful will be to remind your self about expected values and variance, convergence in probability and consistency, maximum likelihood, ordinary least squares and some time series topics like autoregressive models. Also, remind yourself what a dynamic stochastic general equilibrium model is (you probably know more about this than we do, but perhaps not). Less useful are GMM techniques and cointegration.\nBefore coming to class, we suggest that you take a look at the first three (3) chapters of Cosma Shalizi’s preprint Advanced Data Analysis from an Elementary Point of View. These chapters will serve as a nice introduction to the materials we intend to cover. The rest of the book is great too if you are feeling more ambitious/motivated."
  },
  {
    "objectID": "teaching/inet2015.html#preliminary-exercises",
    "href": "teaching/inet2015.html#preliminary-exercises",
    "title": "INET 2015",
    "section": "Preliminary exercises",
    "text": "Preliminary exercises\nWe will make a handful of exercises available to try before you come to class. These are mainly for review and little else. They will certainly not be graded, but spending a few hours trying them may help you come to class more prepared. The exercises are here."
  },
  {
    "objectID": "teaching/inet2015.html#data-analysis",
    "href": "teaching/inet2015.html#data-analysis",
    "title": "INET 2015",
    "section": "Data analysis",
    "text": "Data analysis\nAll of the data analysis examples we use in class will be done in the open source programming language R. R is extremely powerful and easily extensible. We will try to make all of our code available on the website so that you may experiment with it. The software is available for free on CRAN. An official introduction is available there, and many other introductions are available on the web. See for example here. Feel free to download and play around with R, but experience and/or mastery of computer programming is not necessary."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "My primary educational goal is to enhance statistical literacy and communication among my students. To me, statistical literacy means combining careful data analysis with scientific expertise to grapple with problems. Given the quantity of data available today, I want my students to learn how to engage in the type of quantitative reasoning and decision making necessary for understanding applications in the sciences, business, and public policy.\nGoing beyond the didacticism of basic statistical literacy, I strive to inform students about the fundamental role statistics and computation plays in the comprehension of real-world phenomena across disciplines and to motivate them to use this knowledge in their daily lives. Finally, I endeavour to cultivate curiosity in my students and to create an open and nurturing learning environment."
  },
  {
    "objectID": "teaching/index.html#current-courses",
    "href": "teaching/index.html#current-courses",
    "title": "Teaching",
    "section": "Current courses",
    "text": "Current courses\n\nStat 406 Methods for Statistical Learning (2022W1)\nStat 550 Techniques of Statistical Consulting (2022W2)\nStat 548 PhD Qualifying Papers"
  },
  {
    "objectID": "teaching/index.html#links-to-materials-from-old-courses",
    "href": "teaching/index.html#links-to-materials-from-old-courses",
    "title": "Teaching",
    "section": "Links to materials from old courses",
    "text": "Links to materials from old courses\n\nAt UBC\n\n\nStat 535a Topics in Computational Statistics: Convex Optimization (2021W1)\n\n\n\n\nAt Indiana University, Bloomington\n\nS100 Statistical Literacy (Sp20)\nS432 Applied Linear Models II (Sp16) (Sp17) (Sp18)(Sp20)\nS771/S772/785 Seminar on Statistical Theory (Fa16) (Sp17)(Fa19–Sp20)\nS782 Topics in Statistical Learning Theory (Fa17)\n\n\n\nAt the University of Chicago, Booth School of Business\n\n\n41-911 Advanced Econometrics (Aut18)\n\n\n\n\nAt the Institute for New Economic Thinking\n\n\nShort Course on Machine Learning (Sp13) (Sp15) — As part of the Young Scholars Institute at the INET Hong Kong conference from 2–3 April 2013, Darren Homrighausen and I taught a course on statistical learning. I retaught the course in New York, 24–26 February 2015."
  },
  {
    "objectID": "teaching/inet2013.html#instructors",
    "href": "teaching/inet2013.html#instructors",
    "title": "INET 2013",
    "section": "Instructors",
    "text": "Instructors\nDaniel McDonald\nDepartment of Statistics\nIndiana University, Bloomington\nDarren Homrighausen\nDepartment of Statistics\nColorado State Univeristy"
  },
  {
    "objectID": "teaching/inet2013.html#course-description",
    "href": "teaching/inet2013.html#course-description",
    "title": "INET 2013",
    "section": "Course description",
    "text": "Course description\nLargely unnoticed by economists, over the last three decades statisticians and computer scientists have developed sophisticated prediction procedures and methods of model selection and forecast evaluation under the rubric of statistical learning theory. These methods have revolutionized pattern recognition and artificial intelligence, and the modern industry of data mining (without the pejorative connotation) would not exist without it.\nIn this short course, we will investigate the connections between modern statistical methodology and standard techniques which are more common in economics and social science. The goal is to develop an understanding for the circumstances under which different statistical methods are appropriate, how methods behave when the model for the data generating process is incorrect, and why some methods can adapt while others cannot. We will focus especially on selecting models with good predictive performance and on the relationships between model complexity, statistical estimation, and the amount of data used to estimate the model.\nThe starting point for statistical learning is the notion of predictive risk and the tradeoff between bias and variance. With this tradeoff in mind, we will investigate the importance of assessing models using training and test data, the benefits of regularization, and the necessity of selecting tuning parameters carefully. We will illustrate each of these issues with some standard econometric procedures applied to financial and economic datasets while at the same time, introducing some potentially new procedures which may be useful in your own research."
  },
  {
    "objectID": "teaching/inet2013.html#schedule-of-topics",
    "href": "teaching/inet2013.html#schedule-of-topics",
    "title": "INET 2013",
    "section": "Schedule of topics",
    "text": "Schedule of topics\n\nThe predictive viewpoint\nThe bias-variance tradeoff\nEvaluating predictions and estimators\nThe benefits of regularization\nModel selection\nChoosing tuning parameters\nApplication: BVARs and DSGEs\nTools for classification\nApplication: Predicting recessions\nCollaborative filtering and the Netflix prize\nDimension reduction"
  },
  {
    "objectID": "teaching/inet2013.html#suggested-preparation",
    "href": "teaching/inet2013.html#suggested-preparation",
    "title": "INET 2013",
    "section": "Suggested preparation",
    "text": "Suggested preparation\nIn order to be best prepared for this course, we suggest that you brush up on your econometrics, especially the content about probability. Most useful will be to remind your self about expected values and variance, convergence in probability and consistency, maximum likelihood, ordinary least squares and some time series topics like autoregressive models. Also, remind yourself what a dynamic stochastic general equilibrium model is (you probably know more about this than we do, but perhaps not). Less useful are GMM techniques and cointegration.\nBefore coming to class, we suggest that you take a look at the first three (3) chapters of Cosma Shalizi’s preprint Advanced Data Analysis from an Elementary Point of View. These chapters will serve as a nice introduction to the materials we intend to cover. The rest of the book is great too if you are feeling more ambitious/motivated."
  },
  {
    "objectID": "teaching/inet2013.html#preliminary-exercises",
    "href": "teaching/inet2013.html#preliminary-exercises",
    "title": "INET 2013",
    "section": "Preliminary exercises",
    "text": "Preliminary exercises\nWe will make a handful of exercises available to try before you come to class. These are mainly for review and little else. They will certainly not be graded, but spending a few hours trying them may help you come to class more prepared. The exercises are here."
  },
  {
    "objectID": "teaching/inet2013.html#data-analysis",
    "href": "teaching/inet2013.html#data-analysis",
    "title": "INET 2013",
    "section": "Data analysis",
    "text": "Data analysis\nAll of the data analysis examples we use in class will be done in the open source programming language R. R is extremely powerful and easily extensible. We will try to make all of our code available on the website so that you may experiment with it. The software is available for free on CRAN. An official introduction is available there, and many other introductions are available on the web. See for example here. Feel free to download and play around with R, but experience and/or mastery of computer programming is not necessary."
  },
  {
    "objectID": "teaching/stat548.html",
    "href": "teaching/stat548.html",
    "title": "Stat 548 - Qualifying Papers",
    "section": "",
    "text": "Last updated: 5 September 2022"
  },
  {
    "objectID": "teaching/stat548.html#choosing-a-paper",
    "href": "teaching/stat548.html#choosing-a-paper",
    "title": "Stat 548 - Qualifying Papers",
    "section": "Choosing a paper",
    "text": "Choosing a paper\nAt the end of this document is a list of papers and project ideas that I am interested in supervising as Qualifying Papers (QPs). I am happy to discuss any other paper that you are interested in and think might be appropriate. I am generally interested in theoretical and methodological aspects of statistics and machine learning, especially those that relate to regularization, optimization, model selection, and time series forecasting."
  },
  {
    "objectID": "teaching/stat548.html#expectations",
    "href": "teaching/stat548.html#expectations",
    "title": "Stat 548 - Qualifying Papers",
    "section": "Expectations",
    "text": "Expectations\nIf you are interested in doing a QP with me, the first step is to email me to schedule a one-on-one meeting. Please use the words “Qualifying Paper” in the subject line of your email. At our first meeting, please be prepared to discuss:\n\nYour background.\nYour long-term research interests (it’s okay if these are not yet well-defined).\nWhy you are interested in the particular paper/project.\nWhen you will submit your report (typically about four-six weeks after we meet).\nThe details of the QP project and report. * Any concerns you may have."
  },
  {
    "objectID": "teaching/stat548.html#report",
    "href": "teaching/stat548.html#report",
    "title": "Stat 548 - Qualifying Papers",
    "section": "Report",
    "text": "Report\nThe report should have the following structure:1\n\nSummary (~3 pages): The first section of the report should provide a summary of the paper and the problem(s) it addresses, including its relationship to any previous work, its major contributions (e.g., novel techniques, algorithmic developments, problem formulations, theoretical contributions), and any limitations or shortcomings (e.g., restrictive assumptions, computational constraints, flawed methodology). The aim of this section is for you to synthesize the findings of a body of work and clearly present the important points.\nMini-proposals for research projects Each proposal should describe a research project that applies, extends, generalizes, adapts, or addresses shortcomings of the QP. Seemingly unrelated ideas inspired by the original QP are also fine. You may write more than one proposal, but you must write at least one. A proposal should concisely describe: the primary problem to be addressed; an approach (or multiple approaches) for addressing the problem; any technical or conceptual sub-problems; the potential impact of the project. You are not expected to pursue any of these projects (though we can talk more if you would like to). The aim of this section is to get you thinking creatively about research, and to begin developing the skills necessary for writing research proposals. Each proposal should be no more than 2 pages max.\nQP specific project results Each potential QP listed below has a brief description of a related project. We will discuss the project in detail in our initial meeting, and we can meet again (as many times as necessary) before the report due date. Your grade will not be affected by how good the results look, whether your approach improves on past work, or whether you achieve the initial goal of the project. I will use this project to evaluate your research potential, which includes (among other aspects):\n\nclearly formulating a research question;\nsetting up a useful mathematical framework for the problem;\nthinking creatively and independently to develop a solution;\nrelating the problem to existing work, in other fields if necessary;\nbeing resourceful and asking questions when necessary;\nlearning from and moving past the inevitable setbacks;\nreformulating the research problem when necessary;\nimplementing new methods in code (when applicable);\nchoosing appropriate experiments and metrics;\ncommunicating and reflecting on progress, setbacks, and results;\nthinking of future research directions.\n\n\nThe report should be submitted as a GitHub repository based on the template here. The template includes a LATEX style file that should be used for the report. (Detailed instructions for usage can be found in the repository’s README file.) Any experimental/numerical results should be reproducible. All code should be reusable, clearly commented/documented, and exist in the src/ folder of the same GitHub repository to which you give me access as a collaborator. Code can be in any language you wish, though my strong preference is for R or python."
  },
  {
    "objectID": "teaching/stat548.html#resources",
    "href": "teaching/stat548.html#resources",
    "title": "Stat 548 - Qualifying Papers",
    "section": "Resources",
    "text": "Resources\nSome resources on technical/mathematical writing:\n\nNancy Heckman’s page on writing\nHarry Joe’s advice and writing resources for 548\nTrevor Campbell’s How to Explain Things talk\nKnuth, Larrabee, and Roberts on mathematical writing\nJenny Bryan’s Happy Git with R\nGetting started with Git: chapters 1 and 2 should be all you need for this report"
  },
  {
    "objectID": "teaching/stat548.html#available-papers",
    "href": "teaching/stat548.html#available-papers",
    "title": "Stat 548 - Qualifying Papers",
    "section": "Available papers",
    "text": "Available papers\n\n\n\n\nMetzler, Mousavi, Heckel, and Baraniuk. Unsupervised Learning with Stein’s Unbiased Risk Estimator\nThemes: Deep learning, risk estimation, computation.\nProject: Describe and implement SURE for ridge regression using the Auto Differentiation idea discussed in this paper.\nKoyama, Castellanos Pérez-Bolde, Shalizi, and Kass. Approximate Methods for State-Space Models\nThemes: Time series, deconvolution, computation.\nProject: Implement the methodology in reasonable, generalizable software. Use it to estimate the latent states from a simulated non-linear state-space model.\nGreen, Balakrishnan, and Tibshirani. Minimax Optimal Regression over Sobolev Spaces via Laplacian Eigenmaps on Neighborhood Graphs\nThemes: Nonparametric regression, manifolds, sufficient dimension reduction.\nProject: Discuss how the ideas in this paper relate to recent work on “Sufficient Dimension Reduction” as in, e.g., this paper and references therein."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Daniel J. McDonald",
    "section": "",
    "text": "Daniel is Associate Professor of Statistics at the University of British Columbia in Vancouver. His research interests involve the estimation and quantification of prediction risk, especially for structured or dependent data. This includes the application of statistical learning techniques to time series prediction problems for neuroscience, genetics, economics, and epidemiology.\nAssociate Professor Department of Statistics University of British Columbia\n3106 Earth Sciences Building 2207 Main Mall Vancouver, BC V6T 1Z4 Canada"
  },
  {
    "objectID": "other.html",
    "href": "other.html",
    "title": "Links, resources and additional information",
    "section": "",
    "text": "Delphi Research Group at Carnegie Mellon University\n\nWebsite\nCovidcast\nGithub Org\n\nBC COVID-19 Modelling Group\nBC COVID Case Tracker, somewhat useless now that cases are dramatically underreported, but still running."
  },
  {
    "objectID": "other.html#r-packages-for-recent-papers-and-publications",
    "href": "other.html#r-packages-for-recent-papers-and-publications",
    "title": "Links, resources and additional information",
    "section": "R packages for recent papers and publications",
    "text": "R packages for recent papers and publications\n(See github/dajmcdon or my publications page for others)\n\n{fkf}: Fast Kalman Filter. Very fast Kalman filtering and smoothing.\n{sparsegl} Sparse Group Lasso. Efficient implementation of sparse group lasso with optional bound constraints on the coefficients. It supports the use of a sparse design matrix as well as returning coefficient estimates in a sparse matrix. Furthermore, it correctly calculates the degrees of freedom to allow for information criteria rather than cross-validation with very large data. Finally, the interface to compiled code avoids unnecessary copies and allows for the use of long integers.\n{dpf}: Discrete particle filtering. This package greedily estimates switching Kalman filters fast. Also useful for analysis of musical tempos.\n{AIMER}: Amplified, Initially Marginal, Eigenvector Regression. As described in Ding, L. and McDonald, D.J., “Predicting phenotypes from microarrays using amplified, initially marginal, eigenvector regression”. A better version of supervised principal components analysis.\n{cplr}: Compressed penalized linear regression. As described in Homrighausen, D. and McDonald, D.J., “Compressed and penalized linear regression.”"
  },
  {
    "objectID": "other.html#additional-resources",
    "href": "other.html#additional-resources",
    "title": "Links, resources and additional information",
    "section": "Additional resources",
    "text": "Additional resources\n\nAdvice for getting a job, somewhat out of date\nMinimal make files\nNice LaTeX tables\n\n\nResources for R and basic analysis\n(see also my course materials on the Teaching page)\n\nR for Data Science, a nice textbook from Hadley Wickham\nHappy Git with R, good exposition on setting up and combining R and Git\nBasic text mining in R\nFix common Github+R issues, the dreaded rpostback askpass error\nggplot2\nThe batchtools package for easy cluster parallelization"
  },
  {
    "objectID": "other.html#artwork-and-other-attributions",
    "href": "other.html#artwork-and-other-attributions",
    "title": "Links, resources and additional information",
    "section": "Artwork and other attributions",
    "text": "Artwork and other attributions\nThe artwork throughout this website is generated using R and additional packages. In principle, it will change randomly whenever the site is updated. Some of these use the aRtsy package built by Koen Derks. Others use some custom functions that I occasionally work on in my spare time. These have benefited from reading Danielle Navarro’s excellent blog and other material, as well as some posts by Tyler Hobbs.\nThis website was built with Quarto along with some custom CSS inspired by the Hugo Apéro theme. Building the site in Quarto removes some flexibility that I used to have with my old Jekyll site, but it also allows for much easier integration of R code.\nIf any of this seems useful, see the underlying Github repo for the code. There are script files that control the artwork as well as autogenerating the list of publications (a massive headache in other static site generators)."
  },
  {
    "objectID": "other.html#personal",
    "href": "other.html#personal",
    "title": "Links, resources and additional information",
    "section": "Personal",
    "text": "Personal\nI still play cello occasionally, and I enjoy listening to classical music whenever I get the chance. I’ve only once successfully connected research and music, but this leaves music to be a nice hobby, a total diversion from academic work.\nI should also point to my partner’s website: Tamara Mitchell. She does great work in Hispanic literature, and we’re lucky to be at the same institution. We also have two wonderful cats that we spoil rotten.\n\n\n\n\n\n\nGigi\n\n\n\n\n\n\n\nMona"
  },
  {
    "objectID": "students.html",
    "href": "students.html",
    "title": "Information for Students",
    "section": "",
    "text": "Rachel Lobay, PhD in Statistics\nJiaping Liu, PhD in Statistics\nWilliam Laplante, MSc in Statistics (co-supervised with Paul Gustafson)\nElvis Cai, MSc in Statistics (co-supervised with Ben Bloem-Reddy)\n\n\n\n\n\nKen Mawer, NSERC USRA (UBC 2022); BSc Student at UBC\nShuyi Tan, MSc in Statistics (UBC 2022); Data Analyst at VGH & UBC Hospital Foundation\nXiaoxuan Liang, MSc in Statistics (UBC 2022); PhD student in Computer Science at UBC\nWei Tang, MSc in Statistics (UBC 2022); Data Scientist at Amazon\nAaron Cohen, PhD Qualifier (IUB 2021); PhD student in Statistics at IUB\nLei Ding, PhD in Statistics (IUB 2020); Data Scientist at Amazon\nRobert Granger, PhD Qualifier (IUB 2020); PhD student in Statistics at IUB\nHaoran Liu, MSc in Statistics (IUB 2020); PhD student in Statistics at IUB\nMackenzie Turner, CEW&T Emerging Scholars REU (IUB 2020); BA student in SPEA at IUB\nArash Khodadadi, MSc in Statistics (IUB 2018); Data Scientist at Advanced Microgrid Solutions\nMichael McBride, BSc in Statistics (IUB 2018); Software Developer at Epic\nJia Wang, MSc in Statistics (IUB 2017); PhD student in Biostatistics at SUNY Buffalo\nLijiang Guo, PhD Qualifier (IUB 2014); PhD student in ISE at IUB"
  },
  {
    "objectID": "students.html#applying-to-ubc-statistics-or-mds-programs",
    "href": "students.html#applying-to-ubc-statistics-or-mds-programs",
    "title": "Information for Students",
    "section": "Applying to UBC Statistics or MDS Programs",
    "text": "Applying to UBC Statistics or MDS Programs\nI generally have no special powers when it comes to being admitted to MSc/PhD or MDS programs. At UBC, students are admitted to the department before choosing an advisor later in their program. If you are interested, I encourage you to consult the department website.\nIf you want to learn more, your best bet is to email the Graduate Admissions Officer."
  },
  {
    "objectID": "students.html#requesting-letters-of-recommendation",
    "href": "students.html#requesting-letters-of-recommendation",
    "title": "Information for Students",
    "section": "Requesting letters of recommendation",
    "text": "Requesting letters of recommendation\nI frequently get requests for letters of recommendation, and I’m generally happy to comply. However, if I taught you for one class two years ago, it is unlikely that I can provide useful information to a recruiter or admissions committee member.\nIn order for me to write, I will need at least two weeks notice before the first deadline. I will also ask you to provide some information. I’ve found that this helps me to write convincing letters.\nFinally, I only agree to write a fixed number of letters per year for students who take courses with me. This is to be fair to you. Sending letters for many students to the same programs dilutes the impact of my letter with admissions committees. This is another reason that I encourage you to ask early. If you are currently enrolled in class with me, I will ask that you attend office hours, ask questions, of in another way give me some information about yourself. I do not write letters that say “This student took my course and got a 91.” This limit does not apply to PhD students or other students doing research projects under my supervision.\n\nA list of all (or as many as you currently know) positions to which you are applying along with deadlines and some idea of how the letter gets there. (Do I email someone or does some system ask me to upload things in an automated fashion?)\nA recent CV or resume that includes a list of courses taken in your major.\nA personal statement or cover letter (likely the one you’re sending in one of the applications).\nAnswers to as many of the following questions as possible:\n\nFor what class(es) have I been the instructor and how did you distinguish yourself in my class(es)? Any particular experiences you remember that I should be aware of that make you look good?\nPlease choose four adjectives that you think describe you well and provide a brief self-recommendation (2–3 sentences).\nWhat are some of your academic accomplishments?\nWhat are some of your nonacademic accomplishments?\nWhat makes me particularly qualified to write a letter for you?\nWhat makes you particularly qualified for these positions?\nWhat are your long-term goals?"
  },
  {
    "objectID": "students.html#scheduling-research-meetings",
    "href": "students.html#scheduling-research-meetings",
    "title": "Information for Students",
    "section": "Scheduling research meetings",
    "text": "Scheduling research meetings\nI have found the following procedure tends to result in more productive meetings. If you would like to meet to discuss your project:\n\nPlease, consult the calendar below.\nSend me a written document and a time to meet via Slack.\n\n\nBest practices\n\nYour proposed time should\n\nbe available in the calendar.\nbegin between 09:00 and 16:00 Monday through Friday\nbe at least 24 hours in the future.\n\nYour document should\n\ncontain a section summarizing takeaway points of our previous meeting including next steps.\ndescribe what you have done to address those points or give concrete reasons for why they could not be addressed.\nif you were asked to read a paper, summarize the paper and relate it to your project.\ndiscuss what you might do next and why.\nlist questions you would like to discuss."
  },
  {
    "objectID": "students.html#public-calendar",
    "href": "students.html#public-calendar",
    "title": "Information for Students",
    "section": "Public calendar",
    "text": "Public calendar"
  },
  {
    "objectID": "research/index.html",
    "href": "research/index.html",
    "title": "Research",
    "section": "",
    "text": "My research sits at the intersection of statistical theory and computer science methodology and is part of the modern ascendancy of mining “big data” to produce fundamentally novel science from complicated datasets. Specifically, I seek to illuminate the role played by the nature and quantity of regularization as a tool for improved scientific understanding.\nThrough this lens, my research can be divided into four intersecting areas: (1) computational approximation methodology, (2) model selection, (3) high-dimensional and nonparametric theory, and (4) applications related to these. My work explores and exploits the connections between these areas rather than approaching them separately—my contributions have been developed out of the pressing need to justify methodology as implemented in applications rather than in a vacuum devoid of empirical motivation. My research program seeks to generate statistical guarantees for the procedures that applied researchers use while also developing methodology for complicated, high-dimensional problems. Within this context, much of my work involves what is referred to as regularization—the process of mathematically balancing complex but meaningful scientific models with a preference for simple fundamental structures."
  },
  {
    "objectID": "research/index.html#recent-papers-and-preprints",
    "href": "research/index.html#recent-papers-and-preprints",
    "title": "Research",
    "section": "Recent papers and preprints",
    "text": "Recent papers and preprints\n\n\n\n\nSparsegl: An R Package for Estimating Sparse Group Lasso\n\nLiang, X, Cohen, A, Heinsfeld, AS, Pestilli, F, et al. \n\nTechnical Report, 2022\n\n\n\n arXiv\n\n\n\n\nSmooth Multi-Period Forecasting with Application to Prediction of COVID-19 Cases\n\nTuzhilina, E, Hastie, TJ, McDonald, DJ, Tay, JK, et al. \n\nTechnical Report, 2022\n\n\n\n arXiv\n\n\n\n\nLess is More: Balancing Noise Reduction and Data Retention in fMRI with Projection Scrubbing\n\nPham, D, McDonald, DJ, Ding, L, Nebel, MB, et al. \n\nTechnical Report, 2022\n\n\n\n arXiv\n\n code\n\n\n\n\nEvaluation of Individual and Ensemble Probabilistic Forecasts of COVID-19 Mortality in the United States\n\nCramer, EY, Ray, EL, Lopez, VK, Bracher, J, et al. \n\nProceedings of the National Academy of Sciences, 119, e2113561119, 2022\n\n\n\n journal\n\n arXiv\n\n\n\n\nSufficient Principal Component Regression for Genomics\n\nDing, L, Zentner, GE, McDonald, DJ\n\nBioinformatics Advances, 2, vbac033, 2022\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\n  View all publications"
  },
  {
    "objectID": "research/index.html#slides-for-recent-talks",
    "href": "research/index.html#slides-for-recent-talks",
    "title": "Research",
    "section": "Slides for recent talks",
    "text": "Slides for recent talks\n\n\n\n\nMarkov switching state space models for uncovering musical interpretation \nYour model is beautiful, but does it predict? \nCOVID-19 Modelling and Forecasting in the US and Canada: A statisticians perspective \nTrend filtering in exponential families \nAlgorithms for Estimating Trends in Global Temperature Volatility \nRegularization, optimization, and approximation: The benefits of a convex combination \nMatrix sketching for alternating direction method of multipliers optimization \nPredicting phenotypes from microarrays using amplified, initially marginal, eigenvector regression"
  },
  {
    "objectID": "research/all-pubs.html",
    "href": "research/all-pubs.html",
    "title": "All papers and preprints",
    "section": "",
    "text": "Sparsegl: An R Package for Estimating Sparse Group Lasso\n\nLiang, X, Cohen, A, Heinsfeld, AS, Pestilli, F, et al. \n\nTechnical Report, 2022\n\n\n\n arXiv\n\n\n\n\nSmooth Multi-Period Forecasting with Application to Prediction of COVID-19 Cases\n\nTuzhilina, E, Hastie, TJ, McDonald, DJ, Tay, JK, et al. \n\nTechnical Report, 2022\n\n\n\n arXiv\n\n\n\n\nLess is More: Balancing Noise Reduction and Data Retention in fMRI with Projection Scrubbing\n\nPham, D, McDonald, DJ, Ding, L, Nebel, MB, et al. \n\nTechnical Report, 2022\n\n\n\n arXiv\n\n code\n\n\n\n\nEvaluation of Individual and Ensemble Probabilistic Forecasts of COVID-19 Mortality in the United States\n\nCramer, EY, Ray, EL, Lopez, VK, Bracher, J, et al. \n\nProceedings of the National Academy of Sciences, 119, e2113561119, 2022\n\n\n\n journal\n\n arXiv\n\n\n\n\nSufficient Principal Component Regression for Genomics\n\nDing, L, Zentner, GE, McDonald, DJ\n\nBioinformatics Advances, 2, vbac033, 2022\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nAn Open Repository of Real-Time COVID-19 Indicators\n\nReinhart, A, Brooks, L, Jahja, M, Rumack, A, et al. \n\nProceedings of the National Academy of Sciences, 118, e2111452118, 2021\n\n\n\n journal\n\n arXiv\n\n\n\n\nCan Auxiliary Indicators Improve COVID-19 Forecasting and Hotspot Prediction?\n\nMcDonald, DJ, Bien, J, Green, A, Hu, AJ, et al. \n\nProceedings of the National Academy of Sciences, 118, e2111453118, 2021\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nFlexible Analysis of TSS Mapping Data and Detection of TSS Shifts with TSRexploreR\n\nPolicastro, RA, McDonald, DJ, Brendel, VP, Zentner, GE\n\nNAR Genomics and Bioinformatics, 3, 1–10, 2021\n\n\n\n journal\n\n pdf\n\n code\n\n\n\n\nMarkov-Switching State Space Models for Uncovering Musical Interpretation\n\nMcDonald, DJ, McBride, M, Gu, Y, Raphael, C\n\nAnnals of Applied Statistics, 15, 1147–1170, 2021\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nCompressed and Penalized Linear Regression\n\nHomrighausen, D, McDonald, DJ\n\nJournal of Computational and Graphical Statistics, 29, 309–322, 2020\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nBook Review: Sufficient Dimension Reduction: Methods and Applications with R\n\nMcDonald, DJ\n\nJournal of the American Statistical Association, 115, NA, 2020\n\n\n\n journal\n\n\n\n\nAlgorithms for Estimating Trends in Global Temperature Volatility\n\nKhodadadi, A, McDonald, DJ\n\nProceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI-19), 2019\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nA Study on Tuning Parameter Selection for the High-Dimensional Lasso\n\nHomrighausen, D, McDonald, DJ\n\nJournal of Statistical Computation and Simulation, 88, 2865–2892, 2018\n\n\n\n journal\n\n arXiv\n\n\n\n\nRademacher Complexity of Stationary Sequences\n\nMcDonald, DJ, Shalizi, CR\n\nTechnical Report, 2017\n\n\n\n arXiv\n\n\n\n\nMinimax Density Estimation for Growing Dimension\n\nMcDonald, DJ\n\nProceedings of the Twentieth International Conference on Artificial Intelligence and Statistics (AISTATS), 54, 194–203, 2017\n\n\n\n journal\n\n arXiv\n\n\n\n\nPredicting Phenotypes from Microarrays using Amplified, Initially Marginal, Eigenvector Regression\n\nDing, L, McDonald, DJ\n\nBioinformatics, 33, i350–i358, 2017\n\n\n\n journal\n\n arXiv\n\n\n\n\nRisk Consistency of Cross-Validation for Lasso-Type Procedures\n\nHomrighausen, D, McDonald, DJ\n\nStatistica Sinica, 27, 1017–1036, 2017\n\n\n\n journal\n\n arXiv\n\n\n\n\nNonparametric Risk Bounds for Time-Series Forecasting\n\nMcDonald, DJ, Shalizi, CR, Schervish, M\n\nJournal of Machine Learning Research, 18, 1–40, 2017\n\n\n\n journal\n\n arXiv\n\n\n\n\nOn the Nystrom and Column-Sampling Methods for the Approximate Principal Components Analysis of Large Data Sets\n\nHomrighausen, D, McDonald, DJ\n\nJournal of Computational and Graphical Statistics, 25, 344–362, 2016\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nDoes Increased Sexual Frequency Enhance Happiness?\n\nLoewenstein, G, Krishnamurti, T, Kopsic, J, McDonald, DJ\n\nJournal of Economic Behavior and Organization, 116, 206–218, 2015\n\n\n\n journal\n\n\n\n\nEstimating Beta-Mixing Coefficients via Histograms\n\nMcDonald, DJ, Shalizi, CR, Schervish, M\n\nElectronic Journal of Statistics, 9, 2855–2883, 2015\n\n\n\n journal\n\n arXiv\n\n code\n\n\n\n\nLeave-One-Out Cross-Validation is Risk Consistent for Lasso\n\nHomrighausen, D, McDonald, DJ\n\nMachine Learning, 97, 65–78, 2014\n\n\n\n journal\n\n arXiv\n\n\n\n\nThe Lasso, Persistence, and Cross-Validation\n\nHomrighausen, D, McDonald, DJ\n\nProceedings of the Thirtieth International Conference on Machine Learning (ICML), 28, 1031–1039, 2013\n\n\n\n journal\n\n arXiv\n\n\n\n\nGeneralization Error Bounds for State-Space Models\n\nMcDonald, DJ\n\nPh.D. Dissertation, 2012\n\n\n\n pdf\n\n\n\n\nThe Impact of Price Discounts and Calorie Messaging on Beverage Consumption: A Multi-Site Field Study\n\nJue, JJ, Press, MJ, McDonald, DJ, Volpp, KG, et al. \n\nPreventive Medicine, 55, 629–633, 2012\n\n\n\n journal\n\n\n\n\nSpectral Approximations in Machine Learning\n\nHomrighausen, D, McDonald, DJ\n\nTechnical Report, 2011\n\n\n\n arXiv\n\n\n\n\nEstimated VC Dimension for Risk Bounds\n\nMcDonald, DJ, Shalizi, CR, Schervish, M\n\nTechnical Report, 2011\n\n\n\n arXiv\n\n\n\n\nGeneralization Error Bounds for Stationary Autoregressive Models\n\nMcDonald, DJ, Shalizi, CR, Schervish, M\n\nTechnical Report, 2011\n\n\n\n arXiv\n\n\n\n\nEstimating Beta-Mixing Coefficients\n\nMcDonald, DJ, Shalizi, CR, Schervish, M\n\nProceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (AISTATS), 15, 516–524, 2011\n\n\n\n journal\n\n arXiv\n\n\n\n\nThe Behavior of Weight-Loss Study Participants in Response to Incentives\n\nMcDonald, DJ, Loewenstein, GF, Kadane, J\n\nTechnical Report, 2009\n\n\n\n pdf"
  }
]